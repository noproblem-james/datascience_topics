{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the top machine learning frameworks and libraries in python?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could look at Google Trends or Stack Overflow Tags, but here is my general sense:\n",
    "In python:\n",
    "- tensorflow\n",
    "- scikitlearn\n",
    "- sparkml\n",
    "- nltk\n",
    "- keras\n",
    "- pytorch\n",
    "\n",
    "There are other libraries are often *necessary* for machine learning projects, although I'm not sure if I would call them \"machine learning libraries\":\n",
    "- numpy\n",
    "- pandas\n",
    "- matplotlib/seaborn\n",
    "- scipy\n",
    "- statsmodels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Machine Learning? ...vs. AI? ..vs. Deep Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Machine learning** refers to a family of algorithms that teach a computer to recognize patterns in data by example, rather than programming it with specific rules, and then use those patterns to make predictions.\n",
    "\n",
    "**Deep learning** - A specific class of machine learning algorithms that use deep neural networks. Neural networks are a type of computing architecture loosely based on the human brain, and deep neural networks use multiple layers of neurons to hierarchically define features. It is typically used for complex problems like image classification and speech recognition.\n",
    "\n",
    "**AI** - Artificial intelligence. Broadly speaking, AI refers to the ability of machines to simulate intelligent human decision making. It encompasses Machine Learning, Deep learning, and much else. Strong AI, which you often see in science fiction, has the capacity to understand or learn any intellectual task that a human being can or as the ability to perform \"general intelligent action\". Right now, we only have \"weak AI\", which can perform some specific human tasks as well as or better than humans.\n",
    "\n",
    "**Data Science** - The intersection of computer science and statistics. I would say it incorporates all of the above, along with business acumen and/or domain knowledge, the rigorous application of the scientific method, and an empirical approach to problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised vs. Unsupervised Learning problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Supervised learning**: we have target values we are trying to predict, either continuous (regression problem) or categorical (classification problem)\n",
    " - Regression: Can we predict sale price of a home? The Boston House Prices dataset is a famous example of a data set geared toward a regression problem where the inputs are variables that describe a neighborhood and the output is a house price in dollars.\n",
    "   \n",
    " - Classification: Can we predict handwritten numbers? The MNIST handwritten digits dataset is a famous example of a classification problem: the inputs are images of handwritten digits (pixel data) and the output is a class label for what digit the image represents (numbers 0 to 9).\n",
    "\n",
    "\n",
    "**Unsupervised learning**: we don't have target values we are trying to predict. These are often clustering problems or dimensionality-reuduction problems.\n",
    " - Customer segmentation: Which groups customers are most similar to each other?\n",
    " - Data Compression: Can we compress this data while preserving some of its meaning (such that we could uncompress it and not lose too much information)?\n",
    " - Cocktail party problem - Can we separate a single audio stream with two or more different speakers into separate audio streams, each with only one speaker?\n",
    " - Style transfer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification vs. Regression Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Classification problems predict classes (e.g., `hot dog` or `not hot dog`; `Survived` or `Didn't Survive` the Titanic Disaster)\n",
    "- Regression problems predict a continuous variable (e.g., sale price at auction for heavy machinery, barrels of oil produced by an oil well)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Techniques\n",
    "Classification algorithms are machine learning techniques for predicting which category the input data belongs to. \n",
    "Cases\n",
    "* Predicting a clinical diagnosis based upon symptoms, laboratory results and historical diagnosis.\n",
    "* Predicting whether a healthcare claim is fraudulent using data such as claim amount, drug predisposition, disease and provider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curse of Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*When the number of features is large, there tends to be a deterioration in the performance _local approaches_  (e.g., KNN) that make predictions using only observations that are near the test observation for which a prediction must be made.*\n",
    "\n",
    "This decrease in performance results from the fact that in higher dimensions, there is effectively a reduction in sample size. There are no neighbors nearby in the high-dimensional space.\n",
    "\n",
    "Consider the hypercube."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias/Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Models with a lower bias in parameter estimation have a higher variance of the parameter estimates across samples, and vice versa.*\n",
    "\n",
    "- We don't know the true parameters, β, we have to estimate them from the sample. \n",
    "- In the Ordinary Least Squares (OLS) approach, we estimate them as β̂  in such a way, that the sum of squares of residuals is as small as possible.\n",
    "\n",
    "- **Bias** is the difference between the true population parameter and the expected estimator. It measures the accuracy of the estimates. \n",
    "- **Variance**, on the other hand, measures the spread, or uncertainty, in these estimates. where the unknown error variance σ2 can be estimated from the residuals as\n",
    "\n",
    "There are three sources of error coming out of a model:\n",
    "1. Bias error (underfitting) - algo is not capturing the relevant relationships between X and y\n",
    "2. Variance error (overfitting) - algo is memorizing random noise in the training data\n",
    "3. Irreducible error - resulting from noise in the problem itself\n",
    "\n",
    "The OLS estimator has the desired property of being unbiased. However, it can have a huge variance. Specifically, this happens when:\n",
    "- The predictor variables are highly correlated with each other;\n",
    "- There are many predictors. As the number of features approaches the number of observations, the variance approaches infinity.\n",
    "\n",
    "The general solution to this is: *reduce variance at the cost of introducing some bias*. This approach is called regularization and is almost always beneficial for the predictive performance of the model.\n",
    "\n",
    "Although the OLS solution provides non-biased regression estimates, the lower variance solutions produced by regularization techniques provide superior MSE performance.\n",
    "\n",
    "Dimensionality reduction and feature selection can decrease variance by simplifying models. Similarly, a larger training set tends to decrease variance. Adding features (predictors) tends to decrease bias, at the expense of introducing additional variance. Learning algorithms typically have some tunable parameters that control bias and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization is the process of adding information to a model's objective function in order to solve an ill-posed problem or to prevent overfitting.\n",
    "\n",
    "The underlying motivation is often to improve the generalizability of a learned model.\n",
    "\n",
    "#### Ridge regression\n",
    "- Ridge regression is one form of a regularized linear regression model.\n",
    "we not only minimize the sum of squared residuals but also penalize the size of parameter estimates.\n",
    "- penalizes *sum of squared coefficients* (the so-called L2 penalty) each Beta coefficient also has a penalty factor (λ)\n",
    "- Setting λ to 0 is the same as using the OLS, while the larger its value, the stronger is the coefficients' size penalized.\n",
    "- Ridge enforces the β coefficients to be lower, but it *does not enforce them to be zero*. That is, it will not get rid of irrelevant features but rather minimize their impact on the trained model.\n",
    "\n",
    "#### LASSO method\n",
    "Least Absolute Shrinkage and Selection Operator\n",
    "- Similar conceptually to ridge regression--also adds a penalty to each coefficient.\n",
    "- L1 Penalty penalizes the sum of the *absolute values* of coefficients \n",
    "- Can zero out coefficients: For high values of λ, many coefficients are exactly zeroed under lasso, which is never the case in ridge regression.\n",
    " - ctually setting them to zero if they are not relevant. Therefore, you might end up with fewer features included in the model than you started with\n",
    "\n",
    "\n",
    "Lasso is another extension built on regularized linear regression, but with a small twist. The loss function of Lasso is in the form:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### entropy and information gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List at least 5 feature engineering and feature selection techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection Techniques:\n",
    "##### Filter Methods:\n",
    "- **Univariate** (SelectKbest): consider each feature individually and using test statistic (such as chi-squared or pearsons r, if the target is continuous), pick those features most closely associated with the target.\n",
    "- **Variance thresholding**: eliminate features with low levels of variance, because these cannot provide much signal to a model.\n",
    "- **Correlated Feature Elimination**: begin with the pairwise correlation coefficients of all features, sorted by the absolute value of their correlation coefficients. For the most correlated pair of features, remove the one that is less correlated on average with all other features. Recalculate average correlations and continue until no two features are correalted above some threshold (e.g., no two features have a correlation coefficient with an absolute value above .9).\n",
    "\n",
    "##### Wrapper Methods:\n",
    "- **Forward Selection**: Start with an empty set of features, add features one-by-one to reach an optimal model. **(flesh this out more)**\n",
    "- **Backward Elimination**: Start with the full set of features, remove features one-by-one to reach an optimal model. **(flesh this out more)**\n",
    "- **Recursive Feature Elimination**: Perform a greedy search to find the best performing feature subset. In the worst case, if a dataset contains N number of features RFE will do a greedy search for 2^N combinations of features.\n",
    "***\n",
    "#### Feature Engineering Techniques:\n",
    "- Create bins of continuous features\n",
    "- Perform group-by-aggregate operations\n",
    "- Create arithmetic feature interactions (e.g., new_feature = feature_A * feature_B)\n",
    "- Use a clustering algorithm to assign clusters of observations (being sure to exclude the target from this step)\n",
    "- For datetime, extract date parts (e.g., year, month, and day-of-week), or calculate ranges between dates.\n",
    "- If data are spatial, you can do a lot with basic geometry, such as using the physical distance to a nearest-neighbor\n",
    "- (There are endless possibilities, and domain knowledge should definitely play a role in suggesting new possible features.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a decision tree? How does a decision tree decide to make a split?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This splitting algorithm considers each of the features in turn, and for each feature selects the value of that feature that minimizes the impurity of the bins. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How would you go about building a bayesian model?\n",
    "### How does a random forest work?\n",
    "### How does gradient boosting work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Combining the results of multiple models to create more stable and accurate predictions.\n",
    "- Two examples of decision tree ensembles are random forests and gradient-boosted trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging and boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Both Bagging and Boosting are *ensemble techniques*.\n",
    "- Bagging is *parallel*, boosting is *sequential*.\n",
    "\n",
    "#### Bagging\n",
    "- Short for \"bootstrap aggregation\"\n",
    "- Divide the training set into multiple partitions, train models separately on each partition, and then use the average predictions from all models to make a final prediciton.\n",
    "\n",
    "#### Boosting\n",
    "- A model is first trained on all of the training data and predictions are made on the same data. \n",
    "- Predictions that the model got (the most) wrong are then \"up-weighted\", and then the model is trained again so it \"focuses\" on trying to improve the predicitons it got wrong in the previous iteration. \n",
    "- The process is repeated until some stopping condition is reached. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Plain English*: \n",
    "- Principal component analysis (PCA) is a method, taken from linear algebra, that provides a lower-dimensional representation of a dataset.\n",
    "\n",
    "*More technical*: \n",
    "- PCA takes a dataset (matrix)\n",
    "- performs an *orthogonal linear transformation that projects the matrix onto a new coordinate system such that the greatest variance is captured by the first coordinate* (the first principal component), the second-greatest variance is captured by the second coordinate, etc.\n",
    "- You can retain as many principal components as you want to reduce the dimensions arbitrarily.\n",
    "\n",
    "Uses: \n",
    "- *Reveal the latent structure* of a dataset in a way that best explains the variance within it. \n",
    "- *Decorrelate* the features within a dataset to mitigate problem of multicollinearity (while sacrificing some interpretability of the final model built from such a transformed dataset)\n",
    "- *Visualize* high-dimensional datasets, because the human eye can only see in three dimensions (and really only two--think of how difficult it is to read a 3D scatterplot, even if you've done it many times and can mainuplate it in space).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A technique for estimating the generalizability of a predictive model and detecting the problem of overfitting. \n",
    "- The classic approach is *k-folds cross validation*: \n",
    "  - Rather than simply randomly partitioning a dataset into a TRAINING set and a TEST, we do this multiple (k) times. \n",
    "  - In practice, k is usually set to 5 or 10, but this can vary.\n",
    "  - LOOCV (leave one out cross validation) is a special case of k-fold CV in which k is set to equal n.\n",
    "  - there is a bias-variance trade-off associated with the choice of k in k-fold cross-validation. Typically, given these considerations, one performs k-fold cross-validation using k = 5 or k = 10, as these values have been shown empirically to yield test error rate estimates that suffer neither from excessively high bias nor from very high variance.\n",
    "  \n",
    "  Example:\n",
    "  - 5-fold cross validation: A set of n observations is randomly split into five non-overlapping groups. Each of these fifths acts as a validation set , and the remainder as a training set. The test error is estimated by averaging the five resulting MSE estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A Confusion Matrix is a specific type of *contingency table for evaluating the performance of a supervised classification algorithm*, so named because it shows if a model is confusing two classes (i.e., commonly mislabeling one as another). \n",
    "- The rows of the table show counts of the predicted class while the columns represent the instances of the actual class (or vice versa).\n",
    "- It gives a more fine-grained view than simple accuracy (the proportion of \"right answers\" out of all guesses) by showing *how* a model failed. \n",
    "- In the simple case with only two classes, positive and negative, the confusion matrix contains four quadrants, which represent counts of:\n",
    "  - True Positives\n",
    "  - False Positives\n",
    "  - True Negatives\n",
    "  - False Negatives\n",
    "\n",
    "#### Precision\n",
    "- TP / (TP + FP)\n",
    "- The fraction of records that were positive from the group that the classifier predicted to be positive\n",
    "- Also called positive predicted value\n",
    "\n",
    "#### Recall\n",
    "- TP / (TP + FN)\n",
    "- The fraction of positive examples the classifier got right\n",
    "- Also called sensitivity, hit rate, or true positive rate (used in ROC curve, see below)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC-AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Receiver Operator Characteristic - Area Under Curve** \n",
    "- Metric used in machine learning for evaluating and comparing supervised classification algorithms.  \n",
    "- A receiver operator characteristic curve is a graphical plot that shows the performance of a binary classifier as its discrimination threshold is varied. \n",
    "- The ROC curve is the **sensitivity** (True Positive Rate, or *recall*-- see above) as a function of **fall-out** (False Positive Rate, or 1 - specificity)\n",
    "- In theory, the area under the ROC curve varies between 0 and 1, with an uninformative classifier yielding 0.5, no better than random guessing. \n",
    "- A measurement of zero would represent the \"perverse\" case of the model always giving the wrong response, which would be a kind of 'inversely accurate prediction'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unbalanced dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Classification datasets can be “unbalanced\" when there are many observations of one class and few of another. \n",
    "- Accuracy-driven models will over-predict the majority class.\n",
    "  - If a dataset of credit card transactions is 99.9% NOT FRAUD and 0.1% FRAUD, then a model that makes a prediction of NOT FRAUD *every time* will be 99.9% accurate while missing every single case of fraud.\n",
    "\n",
    "#### Possible solutions:\n",
    "- **Downsample/Upsample**\n",
    "  - undersampling consists in sampling from the majority class in order to keep only a part of these points\n",
    "  - oversampling consists in replicating some points from the minority class in order to increase its cardinality\n",
    "- **Generate Synthetic data**: create new synthetic points from the minority class (see SMOTE method for example) to increase its cardinality\n",
    "- **Cost-sensitive learning**: \n",
    "  - Incorporate the monetary costs of a \"miss\" into the objective function. \n",
    "  - For instance, in the context of fraud detection, there are relatively few fraud events, making an unbalanced dataset. \n",
    "  - We know however that the costs of a false negative are often much higher than the costs of a false positive\n",
    "    - i.e., failing to detect an instance of fraud can be more costly than incorrectly flagging a legitimate transaction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Hyperparameter optimization*: choosing a set of optimal hyperparameters for a machine learning algorithm. \n",
    "- A hyperparameter is a variable which controls the learning process (e.g., neuron weights or learning rates) rather than the patterns actually learned (i.e., parameters). \n",
    "- HPO is guided by some performance metric, typically measured by cross-validation on the training set. \n",
    "- One approach to HPO is *grid search*, which is an exhaustive search through all possible combinations of manually specified values for different hyperparameters.  \n",
    "  - It can result in combinatorial explosion, because it trains and evaluates a model for every combination of suggested values of the hyperparameters. \n",
    "  - However, it can usually be easily parallelized, because each model using a specific combination of hyperparameter values is typically independent of all other models built using different combinations.\n",
    "- An alternative approach is Bayesian Optimization, which operates sequentially, meaning that the hyperparameters chosen at the next step will be influenced by the performance of all the previous steps. Bayesian Optimization makes principled decisions about how to balance exploring new regions of the parameter space vs exploiting regions that are known to perform well. It’s generally much more efficient to use Bayesian Optimization than alternatives like Grid Search and Random Search.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, a data pipeline is an object that takes data as input and produces data as output and with a series of specific operations in between. There are *data processing* pipelines that perform Extract, Transform and Load (ETL) operations, and there are also *machine learning* pipelines that take raw features as input and produce predictions as as output. A machine learning pipeline may also include steps such as preprocessing, feature extraction, dimensionality reduction, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Informally, an index refers to the order in which data are organized for easy reference or access.\n",
    "\n",
    "In SQL-speak:\n",
    "Indexing means creating an index--a pointer to data in a table, a special lookup table that the database search engine can use to speed up data retrieval.\n",
    "\n",
    "In pandas-speak: \n",
    "Indexing, or reindexing a series (and by extention a dataframe), would be to assign alter the index, or axis labels, which are technically immutable n-dimensional arrays implementing an ordered, sliceable set. Rows and columns can be thought of as hash-like structures, where the keys are the index and the values are arrays. So indexing is specifiying the values to use as the keys, which we interact with as row labels and column labels. Indexes can be hierarchically structured using multi-indexes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is EDA? or What do you do when you first get a dataset?\t....or Walk me through your EDA process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA stands for Exploratory Data Analysis  \n",
    "Fundamental components (in my opinion):\n",
    "- missingness\n",
    "- dispersion\n",
    "- outliers/anomalies - detection\n",
    "- correlation of features with each other\n",
    "- correlation of features with the target.\n",
    "\n",
    "My approach involves the following steps:\n",
    "1. Examine head and tail of dataframe, datatypes of columns, numbers of unique values per column\n",
    "1. Missingness:\n",
    " 1. Counts of missing (null/NaN) values\n",
    " 1. Create a datafarme of Boolean Missingness Flags\n",
    " 1. Examine pairwise correlations of missingness flag for all columns\n",
    "1. Split dataframe into continuous and categorical features\n",
    "  1. Possibly also deal with datetime and free-text, which are topics unto themselves.\n",
    "1. Continuous features:\n",
    " 1. Simple descriptive statistics (mean, median, min, max, quartiles, skewness) for each column\n",
    " 1. Correlation matrix (optionally visualized as a heatmap, ordered according to a hierarchical clustering algorithm.)\n",
    " 1. Outlier detection, using robust methods:\n",
    "    1. MAD - median absolute deviation\n",
    "    2. Mahalanobis distance\n",
    "  1. Visualize continuous distributions, using univariate and covariate plots:\n",
    "   1. Scattermatrix of subsets of continuous features (look at it before and after dropping outliers)\n",
    "   1. Strip plots and/or boxplots of single features of interest\n",
    "   1. Scatterplots of each continuous feature with the target\n",
    "1. Categorical features:\n",
    "  1. Bar charts representing value-counts of categorical features.\n",
    "  1. Look at frequency counts - consider ways of rolling up\n",
    "  1. Look at correlation w\n",
    "1. Maps, line plots with time, if you have a spatial or temporal problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do you handle Missing Values?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem of missing values is both *common and insidious*, because it *introduces bias* into everything it touches.   \n",
    "\n",
    "**Examples** include:\n",
    "- Non-observed population segments\n",
    "- Participant dropout in longitudinal studies\n",
    "- Malfuncitoning sensors\n",
    "- Network problems in data transfer\n",
    "\n",
    "Data can be missing in three different ways (called different **missingness regimes**):\n",
    "1. MCAR - missing completely at random, a truly stochastic process (e.g., your thermomenter randomly fails to record one out of every hundred temperature readings)\n",
    "1. MAR - missing at random, a deterministic but noisy process that removes data based on other data (this is the single most common) (e.g., your thermometer fails when windspeeds are above 15 mph)\n",
    "1. MNAR - Missing not at random - a deterministic but noisy process that removes data based on the data itself (like your thermometer fails at temps above 75 degrees)\n",
    " \n",
    "What you *can't* do:\n",
    "1. Listwise deletion (aka `df.dropna()` in pandas). You can only do this if the data is missing completely at random, and this is impossible to prove theoretically and very unlikely in practice.\n",
    "1. Gather more data. You won’t be saved by big data. As a dataset with missing values gets larger and larger we get closer and closer to the wrong value for our coefficient estimates and this leads to false confidence.\n",
    "1. Just impute the mean. It doesnt matter how the data is missing, it will lead to larger bias in parameter estimates and larger model error.\n",
    " \n",
    "What you *can* do: \n",
    "1. Establish your missingness regime\n",
    "1. Use a modern multiple imputation technique like MICE (multiple imputation by chained equations)\n",
    "1. Use auxillary features… ie features you're not expecting to use in predition model, but you will use in imputation, because they are known to be strongly correlated with the missing data\n",
    "1. Anticipate extra compute time\n",
    "1. Report all the things as a good scientist and good citizen:\n",
    "  - How many rows are missing one or more values\n",
    "  - the fraction of missing values\n",
    "  - the pattern of missing values\n",
    "  - your imputation strategy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Encoding means creating an abstract (mathematical) representation of some entity or phenomenon.\n",
    "- For example, we could encode a digital image as a matrix of RGB values for each pixel or sounds based on their waveform properties\n",
    "- A very common problem in machine learning is how to encode categorical data. Most models only allow numeric values as input.\n",
    "\n",
    "#### Encoding Categorical (Nominal or Ordinal) data\n",
    "- Three approaches:\n",
    "  1. One-hot encoding\n",
    "  1. Feature Hashing\n",
    "  1. Binary Encoding\n",
    "  \n",
    "##### One-hot encoding\n",
    " - (OHE, or dummy variabales) Is the classic approach\n",
    " - a single column with a finite number of categories (string values) is translated into several columns, each representing each possible category, with binary Boolean values (0 for False or 1 for True)  \n",
    "\n",
    "- **Pros**\n",
    "  - Easy to implement\n",
    "  - May not have a lot of negative impacts if cardinality is low.  \n",
    "\n",
    "- **Cons**:\n",
    "  - OHE representation produces very high dimensionality, this causes an increase in the model’s training and serving time and memory consumption.\n",
    "  - OHE can easily cause a model to overfit the data.\n",
    "  - OHE can’t handle categories that weren’t in the training data (like new URLs, new device types etc), this can be problematic in domains that change all the time.\n",
    "    - Can be handled with a catch all \"other\" category\n",
    "  - One-hot-encoded data can also be difficult for decision-tree-based algorithms, especially if the categorical feature has high cardinality:\n",
    "     - Independence From the splitting algorithm's point of view, each binary variable is treated as if theyre all independent, which theyre not.\n",
    "     - Creates a sparse matrix, where each feature has a very few number of \"hot\" rows, and thus not much signal to provide.\n",
    "     - A binary variable can only be split in one place, and a categorical variable with q levels can be split in `2q / 2 −1` ways.\n",
    "\n",
    "\n",
    "##### Feature Hashing\n",
    "- implements the hashing trick\n",
    "- similar to one-hot encoding but with fewer new dimensions and some info loss due to collisions. \n",
    "- collisions do not significantly affect performance unless there is a great deal of overlap.\n",
    "- **Pros** \n",
    "  - It is low dimensional thus it is very efficient in processing time and memory\n",
    "  - it can be computed with online learning because we don’t need to go over all the data and build a dictionary of all possible categories \n",
    "  - mapping is not affected by new kinds of categories.\n",
    "  \n",
    "- ***Cons***  \n",
    "  - Hashing functions sometimes have collision so if H(New York) = H(Tehran), then the model can’t know what city were in the data. \n",
    "   - There are some sophisticated hashing function that try to reduce the number of collisions\n",
    "   - In any case, studies have shown that collisions usually don’t substantially reduce a model's performance. \n",
    "  - Hashed features are not interpretable so doing things like feature importance and model debugging is difficult\n",
    "\n",
    "\n",
    "##### Binary Encoding\n",
    "- Can be thought of as a hybrid of one-hot and hashing encoders. \n",
    "- Creates fewer features than one-hot, while preserving some uniqueness of values in the the column.\n",
    "- Can work well with higher dimensionality ordinal data.\n",
    "\n",
    "```\n",
    "- The categories are encoded by OrdinalEncoder if they aren’t already in numeric form.\n",
    "- Then those integers are converted into binary code, so for example 5 becomes 101 and 10 becomes 1010\n",
    "- Then the digits from that binary string are split into separate columns. So if there are 4–7 values in an ordinal column then 3 new columns are created: one for the first bit, one for the second, and one for the third.\n",
    "- Each observation is encoded across the columns in its binary form.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
