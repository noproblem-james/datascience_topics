{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Algorithms\n",
    "High-level description. Pros, Cons, Pseudocode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Supervised\n",
    "Supervised learning tasks require labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Linear Regression\n",
    "Linear regression attempts to fit a straight hyperplane to your dataset that is closest to all data points. It is most suitable when there are linear relationships between the variables in the dataset.\n",
    "NB: For higher-order terms, although the predictor variables of Polynomial linear regression are not linear the relationship between the parameters or coefficients is linear.\n",
    "### Pros\n",
    "* Quick to compute and can be updated easily with new data\n",
    "* Relatively easy to understand and explain\n",
    "* Regularization techniques can be used to prevent overfitting\n",
    "\n",
    "### Cons\n",
    "* Unable to learn complex relationships\n",
    "* Difficult to capture non-linear relationships (without first transforming data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Logistic Regression\n",
    "Logistic regression predicts the probability of a binary outcome. A new observation is predicted to be within the class if its probability is above a set threshold. There are methods to use Logistic Regression for scenarios where there are multiple classes.\n",
    "### Pros\n",
    "* Quick to compute and can be updated easily with new data\n",
    "* Output can be interpreted as probability; this can be used for ranking\n",
    "* Regularization techniques can be used to prevent overfitting\n",
    "\n",
    "### Cons\n",
    "* Unable to learn complex relationships\n",
    "* Difficult to capture non-linear relationships (without first transforming data which can be complicated)\n",
    "\n",
    "### Pseudocode\n",
    "- Take our features and multiply each by a weight and then sum them up\n",
    "- Feed this weighted sum into a sigmoid function, which will return a number between zero and one\n",
    "  - We can think of this as a probabilty estimate... the probability that the observation belongs to one of the classes\n",
    "- Anything above 0.5 we'll classify as 1 and anything below 0.5, we'll classify as 0.\n",
    "- Now we need to know what are the best weights or regression coefficients to use, and how to find them.\n",
    "  - This is an optimization problem which can be solved in many ways, including by stochastic gradient descent. (see below)\n",
    "  \n",
    "##### Stochastic gradient descent\n",
    "- Set all weights equal to 1\n",
    "- For each piece of data in the dataset:\n",
    " - Calculate the gradient of one piece of data\n",
    " - Update the weights vector by alpha * the gradient\n",
    " - return the weights vector\n",
    "\n",
    "**Pros**:\n",
    "- Computationally inexpensive\n",
    "- Easy to implement\n",
    "- Easy to interpret (we have coefficients, i.e., the weights)\n",
    "\n",
    "**Cons**\n",
    " - Prone to underfitting. May have low accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1.3. K-Nearest Neighbors\n",
    "K-Nearest Neighbor (KNN) makes a prediction for a new observation by searching for the most similar training observations and pooling their values.\n",
    "### Pros\n",
    "* Simple\n",
    "* Powerful\n",
    "* No training involved\n",
    "\n",
    "### Cons\n",
    "* Expensive & slow to predict new instances\n",
    "* Performs poorly on high dimensional datasets\n",
    "\n",
    "### Pseudocode\n",
    "1. Pick a value for K.\n",
    "2. Calculate the distance from the new case (holdout from each of the cases in the dataset).\n",
    "3. Search for the K observations in the training data that are ‘nearest’ to the measurements of the unknown data point.\n",
    "4. predict the response of the unknown data point using the most popular response value from the K nearest neighbors.\n",
    "\n",
    "There are two parts in this algorithm that might be a bit confusing.\n",
    "First, how to select the correct K; and second, how to compute the similarity between cases,\n",
    "for example, among customers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Naïve Bayes\n",
    "Naïve Bayes assumes that all features are independent, that they independently contribute to the probability of the target variable's class; this does not always hold true, which is why it is referred to as \"naive\". Various probabilities and likelihood values are calculated based upon the frequency they appear in the data and the final probabilities calculated using a formula called Bayes Theorem.\n",
    "### Pros\n",
    "* Simple and easy to interpret\n",
    "* Computationally fast\n",
    "* Good for high-dimensional space (lots of features)\n",
    "\n",
    "### Cons\n",
    "* Performance will be inhibited if significant dependence between variables\n",
    "* If a class that appears in the test data did not appear in the training data, it will be given a probability of zerom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Support Vector Machines\n",
    "If you plot your data in an n-dimensional space (where n is the number of features), Support Vector Machines (SVM) attempt to fit a hyperplane that best separates the categories. When you have a new data point, its position in relation to the hyperplane will predict which category the point belongs to.\n",
    "### Pros\n",
    "* High accuracy\n",
    "* Ability to find solutions even if non-linearly separable\n",
    "* Good for high-dimensional space (lots of features)\n",
    "\n",
    "### Cons\n",
    "* Hard to interpret\n",
    "* Can be slow to train large data sets\n",
    "* Memory-intensive\n",
    "\n",
    "The support vector machine is a maximal margin classifier that seeks to construct a hyperplane that linearly separates training observations of one class from the other class(es).\n",
    " \n",
    "**Preprocessing data for SVM**  \n",
    "Intuitively, it makes sense that SVMs might need scaling. Since SVMs are sensitive to the distance of points relative to a hyperplane, if one dimension had units in the thousands, the distance along that dimension would overwhelm another dimension with values in [0,1]. And the model would focus disproportionately on this larger dimension. Scaling overcomes this.\n",
    "\n",
    "**Kernel Trick**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6. Decision Trees\n",
    "Decision trees learn how to best split the dataset into separate branches, allowing it to learn non-linear relationships. Random forests (RF) and Gradient Boosted Trees (GBT) are two algorithms that build many individual decision trees, pooling their predictions. As they use a collection of results to make a final decision, they are referred to as 'Ensemble techniques'.\n",
    "#### Pros\n",
    "* A single decision tree is fast to train\n",
    "* Robust to noise/outliers and missing values\n",
    "\n",
    "#### Cons\n",
    "* Single decision trees are prone to overfitting (which is where ensembles come in!)\n",
    "* Complex trees are hard to interpret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8. Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "Neural networks can learn complex patterns using layers of neurons which mathematically transform the data. The layers between the input and output are referred to as “hidden layers”. A neural network can learn relationships between the features that other algorithms cannot easily discover.\n",
    "#### Pros\n",
    "* Extremely powerful / state of the art for many domains (e.g. computer vision, speech recognition)\n",
    "* Can learn even very complex relationships\n",
    "* Hidden layers reduce need for feature engineering (no need to understand underlying data)\n",
    "\n",
    "#### Cons\n",
    "* Require a very large amount of data!\n",
    "* Prone to overfitting\n",
    "* Long training time\n",
    "* Requires significant computing power for large datasets (computationally expensive)\n",
    "* Model is a \"black box\", unexplainable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Unsupervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Clustering\n",
    "Clustering algorithms are machine learning techniques to divide data into a number of groups where points in groups have similar traits. They are unsupervised learning tasks and therefore do not require labeled data.\n",
    "Example use-cases:  \n",
    "* Segmenting your market based upon similar collections of customers using their location, spending habits and demographics.\n",
    "* Understanding topics in your documents, whether they are emails, reports, or customer call transactions by exploring the common words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1. K-means\n",
    "K-Means Clustering aims to partition the observations into k clusters. The algorithm will determine which observation is in which cluster and also the center of each cluster. A new observation is assiged the cluster whose center it is nearest to.\n",
    "#### Pros\n",
    "* Simple and easy to implement\n",
    "* Easy to interpret results\n",
    "* Fast\n",
    "\n",
    "#### Cons\n",
    "* Sensitive to outliers\n",
    "* You must define the number of clusters\n",
    "* Assumes the clusters are spherical\n",
    "* The clusters are found using a random starting point so may not be repeatable and can require multiple runs to find an optimal solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2. Gaussian Mixture Model\n",
    "With a Gaussian Mixture Model (GMM), we are assuming that the k clusters are of normal distribution. It's algorithm tries to find the mean and standard deviation of each of these clusters. For a new observation, the probability it belongs to each cluster is calculated, resulting in a soft assignment.\n",
    "#### Pros\n",
    "* Does not enforce the clusters to be circular\n",
    "* Points can be assigned to multiple clusters\n",
    "\n",
    "#### Cons\n",
    "* You must define the number of clusters\n",
    "* Difficult to interpret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3. Agglomerative Hierarchical Clustering\n",
    "Agglomerative hierarchical clustering is an algorithm that builds a hierarchy of clusters. Initially all points start as their own cluster, then the two nearest clusters merge as the same cluster. This process continues, clusters merging until only one cluster containing all the data points remains. To identify the significant clusters a threshold would be chosen.\n",
    "#### Pros\n",
    "* Easy probabilistic model with interpretable topics\n",
    "* Does not require defining the number of clusters\n",
    "* Clusters can be arbitrarily shaped\n",
    "\n",
    "#### Cons\n",
    "* Slow and therefore not suitable for big data\n",
    "* Can be hard to identify the correct number of clusters\n",
    "* Interpretation can be confusing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4. DBSCAN\n",
    "Density-Based Spatial Clustering of Applications with Noise (DBSCAN) attempts to find dense areas of data points and identifies these as a cluster. If data points are close enough to each other, and there are a sufficient number of them, they form a cluster. If not, they are labeled as noise and ignored.\n",
    "#### Pros\n",
    "* Can find arbitrarily shaped clusters\n",
    "* Does not require defining the number of clusters\n",
    "* Robust to outliers\n",
    "\n",
    "##### Cons\n",
    "* Cannot cluster datasets with large differences in densities\n",
    "* Can perform poorly on high dimensional data\n",
    "* Choosing the right parameters for density can be difficult\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Detection Techniques\n",
    "Anomaly Detection is a technique used to identify unusual events or patterns that do not conform to expected behavior. Those identified are often referred to as anomalies or outliers.\n",
    "Cases\n",
    "* Detect abnormal behavior of equipment in a manufacturing plant using sensor data such as temperature, pressure and humidity\n",
    "* Detect and prevent fraudulent spending by understanding normal customer spending amounts, locations and time between taransactions\n",
    "\n",
    "Clustering techniques are a common approach for anomaly detection. Clusters of \"normal\" characteristics are identified and if the distance between a new point and all other clusters is too great, it is identified as an anomaly.\n",
    "\n",
    "#### Autoencoder\n",
    "An Autoencoder is a technique for dimensionality reduction. It is a type of neural network where the first part of the network, called the encoder, reduces the input to a lower dimension. The second part of the network, called the decoder, aims to reconstruct the original input. The goal is to create a model where the input and the output are the same. A new data point can be passed through the model and if the error between the input data and the computed output is too great, it can be flagged as an \"anomaly\".\n",
    "#### Pros\n",
    "* Can capture non-linearities and subtle connections between the features\n",
    "* Variations result in state-of-the-art results\n",
    "* If the data is temporal, LSTM (long-short-term-memory) autoencoders can be used\n",
    "Cons\n",
    "* Requires a very large amount of data\n",
    "* Many hyperparameters to tune\n",
    "* Long training time\n",
    "* Requires significant computing power for large datasets\n",
    "\n",
    "#### One-Class Support Vector Machines\n",
    "If you were to plot your data in an n-dimensional space (where n is the number of features), One-Class Support Vector Machines (SVM) attempt to identify the region where most cases lie, these are considered \"normal\". It will then fit a hyperplane that best separates these \"normal\" examples from the rest. When you have a new data point, it is labeled as \"normal\" or an \"anomaly\" depending how close it is to the \"normal\" boundary\n",
    "Pros\n",
    "* No assumptions on the distribution of the data\n",
    "* Ability to find normal boundary that is non-linear\n",
    "* Can be used in high-dimensional space\n",
    "Cons\n",
    "* Choosing the right hyper-parameters to find the appropriate non-linear shape of the boundary can be difficult\n",
    "* Can be slow to train large datasets\n",
    "* Memory intensive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendation Engine Techniques\n",
    "Recommendation Engines are created in order to predict a preference or rating that indicates a users interest in an item/product. The algorithms used to create this system find similarities between either the users, the items or both.\n",
    "Cases\n",
    "* Recommend clothing to a customer based on brands, colors, and price of previously purchased clothing\n",
    "* Recommend a medical treatment for a patient based upon successful treatments given to similar patients using their condition, diagnosis and previous treatment information\n",
    "\n",
    "#### Content-based Recommenders\n",
    "Content-based recommenders suggest similar items to those already liked by the customer, whether explicitly (e.g. by rating) or implicitly (by purchasing). This type of system uses metadata describing the item. ",
    " ",
    "Each item is represented as a vector and a distance metric compares the items' vectors to find the most similar.\n",
    "Pros\n",
    "* Quick to implement\n",
    "* No popularity bias - can recommend new items\n",
    "* Results are interpretable\n",
    "Cons\n",
    "* Important to have meaningful metadata, tagging can be tiresome\n",
    "* \"Cold start problem\" for new users without history of liking items\n",
    "* Limits novelty as won't suggest items too disimilar to previously liked\n",
    "\n",
    "#### Model-based Collaborative Filtering\n",
    "Collaborative filtering is a method of predicting a user's interest by analysing preferences by other users. There are two types, user-based filtering and item-based filtering. ",
    " ",
    "Model-based recommenders use training data to build a model, a mathematical formula that takes the features of an observation and calculates a prediction. There are many algorithms to use, including neural networks, Bayesian networks and matrix factorization.\n",
    "Pros\n",
    "* Fast & scalable; doesn't require the full dataset each time\n",
    "* User-based suggestions can result in a diverse set of suggestions across domains\n",
    "* User-based suggestions do not require metadata\n",
    "Cons\n",
    "* Data sparsity can result in performance issues\n",
    "* Models can be complex and slow to train\n",
    "* \"Cold start problem\" - new items struggle to be recommended (popularity bias) and for new users with no history its hard to make good recommendations\n",
    "\n",
    "\n",
    "#### Memory-based Collaborative Filtering\n",
    "Collaborative filtering is a method of predicting a user's interest by analysing preferences by other users. There are two types, user-based filtering and item-based filtering. ",
    " ",
    "Memory-based filtering computes similarity between users, or items, and uses other users' ratings to make a prediction; a typical approach is a neighborhood-based algorithm. The predicted rating for the user and each item is calcuated using other users' ratings and the similarity distance\n",
    "Pros\n",
    "* Quick to implement\n",
    "* Results are interpretable\n",
    "* User based suggestions can result in a diverse set of suggestions across domains\n",
    "Cons\n",
    "* Data sparsity can result in performance issues\n",
    "* Slow & computationally expensive - requires the whole dataset to make a prediction\n",
    "* \"Cold start problem\" - new items struggle to be recommended (popularity bias) and for new users with little history it's hard to make recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
