{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics Quiz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the Central Limit Theorem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CLT in plain english**: A good sample should look a lot like the population from which it is drawn, and will vary from that population in a predictable way.\n",
    " - i.e., if we took lots of samples of a population, each sample would be slightly different from the others, but samples that deviate greatly from the relevant population would be relatively rare.\n",
    " - For instance, if we took a sample of household income, which is right skewed (pareto distributed), we would expect a mean income of the sample to approximately equal the mean of the population.\n",
    "\n",
    "**More specifically**: If you draw a large, random sample from a population, then the distribution of a sample statistic will be approximately normally distributed around the underlying population parameter.\n",
    "- Regardless of the underlying distribution of the population parameter\n",
    "- This holds true for many frequently **tested statistics**, such as: \n",
    "  - a one-sample mean\n",
    "  - a one-sample proportion\n",
    "  - the difference in two means\n",
    "  - the difference in two proportions\n",
    "  - the slope of a simple linear regression model\n",
    "  - Pearson's r correlation. \n",
    "- So, what constitutes a **sufficiently large** sample size? \n",
    "   - \"Sufficiently large\" is defined as the point at which the sampling distribution becomes approximately normal--which seems circular, because it is.\n",
    "   - A sample size (n) of 30 is often used as a rule of thumb\n",
    "- What is meant by **normally distributed**?\n",
    "   - A symmetric, bell-shaped distribution, common to many  phenomena:\n",
    "     - 68.2% of observations (e.g., sample means) will lie within one standard deviation (in this case, one standard error) of the population mean \n",
    "     - 95.4% will lie within two standard errors of the population mean\n",
    "     - 99.7% within three standard errors\n",
    "   \n",
    "So, the big picture:\n",
    "- If you draw large, random samples form any population, the means of those samples will be distributed normally around the population mean (regardless of what the shape of the underlying distribution looks like).\n",
    "- Most sample means will lie reasonably close to the population mean; the standard error is what defines \"reasonably close\"\n",
    "- The CLT will tell us the probablity that a sample mean will lie within a certain distance of the population mean. It is relatively unlikely that a sample mean will lie more than two standard errors from the population mean, and extremely unlikely that it will lie three or more standard errors from the population mean.\n",
    "- The less likelty it is that an outcome has been observed by chance, the more confiedent we can be in surmising that some other factor is in play (this is the basis of inferential statistics).\n",
    " \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a confidence interval? (and how do you construct one?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confidence interval is a range computed using sample statistics to estimate an unknown population parameter with a stated level of confidence.\n",
    "- Contrasted with a point estimate (\"The president has 45% approval rating.\" vs \"the President has 45% +-3% approval rating.\")\n",
    "- It's just another way of expressing the same interval as a margin of error in political preference polling.\n",
    "  - The margin of error is the range of values below and above the sample statistic in a confidence interval.\n",
    "  - That means if the poll is repeated using the same techniques, about 95% of the time the true population parameter (parameter vs. statistic) will fall within the interval estimates.\n",
    "\n",
    "Example: Statistical Anxiety\n",
    "The statistics professors at a university want to estimate the average statistics anxiety score for all of their undergraduate students. It would be too time consuming and costly to give every undergraduate student at the university their statistics anxiety survey. Instead, they take a random sample of 50 undergraduate students at the university and administer their survey.\n",
    "\n",
    "Using the data collected from the sample, they construct a 95% confidence interval for the mean statistics anxiety score in the population of all university undergraduate students. They are using  to estimate . If the 95% confidence interval for  is 26 to 32, then we could say, “we are 95% confident that the mean statistics anxiety score of all undergraduate students at this university is between 26 and 32.” In other words, we are 95% confident that . This may also be written as .\n",
    "\n",
    "Example 2\n",
    "In a random sample of 525 Penn State World Campus students the mean height was 67.009 inches with a standard deviation of 4.462 inches. \n",
    "The standard error was computed to be 0.195.\n",
    "Construct a 95% confidence interval for the mean height of all Penn State World Campus students.\n",
    "\n",
    "95% confidence interval:\n",
    "\n",
    "67.009 ± 2(0.195)\n",
    "67.009 ± 0.390\n",
    "[66.619, 67.399]\n",
    "\n",
    "I am 95% confident that the mean height of all Penn State World Campus students is between 66.619 inches and 67.399 inches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the steps of a hypothesis test?\n",
    "\n",
    "#### Hypo test at a High level:\n",
    "1. Specify the null hypothesis\n",
    "1. Specify a significance level\n",
    "1. Compute the test statistic and p-value\n",
    "1. Compare the p-value to the significance level and draw a conclusion.  \n",
    "\n",
    "<br />\n",
    "\n",
    "***\n",
    "\n",
    "#### More Detail:\n",
    "\n",
    "1. Specify the null hypothesis. \n",
    "  - This actually first requires that we **formulate a research hypothesis** and **state our assumptions**\n",
    "  - The null hypothesis is the straw man you are trying to kick down. It’s the “no effect” situation. (You secretly want to detect an effect that would support your research hypothesis, but SHHH!)\n",
    "  - For a two-tailed test, the null hypothesis is typically that a parameter equals zero, although there are exceptions.\n",
    "1. Specify a significance level.\n",
    "  - The significance level is also called alpha (α) level.\n",
    "  - Alpha is the probability of making a Type I error - i.e., the probability of incorrectly rejecting the null hypothesis. \n",
    "  - If you have an alpha of .05, then that means you will tolerate a 5% chance that you have detected an effect, even though it didn’t exist.\n",
    "  - You set risk level you’re comfortable with. It’s arbitrary. Typical values are 0.05 and 0.01, but these are purely convention.\n",
    "1. Compute the test statistic and p-value\n",
    "  - The **p-value** is the probability of obtaining a sample statistic as different or more different from the parameter specified in the null hypothesis given that the null hypothesis is true.\n",
    "  - The **test statistic** is the standardized value that is the boundary of the p-value. Recall the formula for a z score: . The formula for a test statistic will be similar. When conducting a hypothesis test the sampling - distribution will be centered on the null parameter and the standard deviation is known as the standard error.\n",
    "   1. A **one-tailed or two-tailed** test?\n",
    "     - Depends on the research question.\n",
    "     - A one-tailed test is where you are only interested in one direction. If a mean is x, you might want to know if a set of results is more than x or less than x. A one-tailed test is more powerful than a two-tailed test, as you aren’t considering an effect in the opposite direction.\n",
    "    - The significance level needs to be halved for the two-tailed test; so if you  want to detect an effect at the 0.5 significance level, only reject the null hypothesis if the p-value is less than 0.025.\n",
    "   1. A **t-test or a z-test**?\n",
    "     - A z-test compares a sample to a defined population and is typically used for dealing with problems relating to large samples (n > 30). Z-tests can also be helpful when we want to test a hypothesis. Generally, they are most useful when the standard deviation is known.\n",
    "    - Like z-tests, t-tests are calculations used to test a hypothesis, but they are most useful when we need to determine if there is a statistically significant difference between two independent sample groups. t-tests are most appropriate when dealing with problems with a limited sample size (n < 30).\n",
    "    - Both z-tests and t-tests require data with a normal distribution, which means that the sample (or population) data is distributed evenly around the mean\n",
    "    - A t score is similar to a z score — it represents the number of standard deviations from the mean. While the z-score returns values from between -5 and 5 (most scores fall between -3 and 3) standard deviations from the mean, the t score has a greater value and returns results from between 0 to 100 (most scores will fall between 20 and 80). Many people prefer t scores because the lack of negative numbers means they are easier to work with and there is a larger range so decimals are almost eliminated. \n",
    "   1. **Paired or unpaired**?\n",
    "    - Paired tests are used when the samples are dependent; that is, when there is only one sample that has been tested twice (repeated measures) or when there are two samples that have been matched or \"paired\". \n",
    "    - For example, 21 patients blood pressure is recorded before and after treatment. We want to investigate the claim that the treatment will modify the patients' blood pressure.\n",
    "   \n",
    "1. Compare the p-value to the significance level, and make a decision\n",
    " - If the probability value is lower than your pre-specified significance level, then you reject the null hypothesis.\n",
    " - You either reject the null hypothesis or fail to reject the null hypothesis.\n",
    " - Failure to reject the null hypothesis does not constitute support for the null hypothesis. It just means you do not have sufficiently strong data to reject it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is statistical power?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regresssion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression\n",
    "- Take our features and multiply each by a weight and then sum them up\n",
    "- Feed this weighted sum into a sigmoid function, which will return a number between zero and one\n",
    "  - We can think of this as a probabilty estimate... the probability that the observation belongs to one of the classes\n",
    "- Anything above 0.5 we'll classify as 1 and anything below 0.5, we'll classify as 0.\n",
    "- Now we need to know what are the best weights or regression coefficients to use, and how to find them.\n",
    "  - This is an optimization problem which can be solved in many ways, including by stochastic gradient descent. (see below)\n",
    "  \n",
    "##### Stochastic gradient descent\n",
    "- Set all weights equal to 1\n",
    "- For each piece of data in the dataset:\n",
    " - Calculate the gradient of one piece of data\n",
    " - Update the weights vector by alpha * the gradient\n",
    " - return the weights vector\n",
    "\n",
    "**Pros**:\n",
    "- Computationally inexpensive\n",
    "- Easy to implement\n",
    "- Easy to interpret (we have coefficients, i.e., the weights)\n",
    "\n",
    "**Cons**\n",
    " - Prone to underfitting. May have low accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANOVA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametric and Nonparametric Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the assumptions of linear regression models? How can you test for violaiton of those assumptions? What are the remedies?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified sampling, other sampling procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a Maximum Likelihood Estimator?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Maximum likelihood estimation** (MLE) is a method of estimating the parameters of a probability distribution by maximizing a likelihood function, so that under the assumed statistical model the observed data is most probable. \n",
    "- The point in the parameter space that maximizes the likelihood function is called the maximum likelihood estimate.\n",
    "- The logic of maximum likelihood is both intuitive and flexible, and as such the method has become a dominant means of statistical inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###How do you handle Missing Values?\n",
    "\n",
    "The problem of missing values is both *common and quite insidious*, because *introduces bias* into everything it touches.   \n",
    "\n",
    "**Examples** include:\n",
    "- Non-observed population segments\n",
    "- Participant dropout in longitudinal studies\n",
    "- Malfuncitoning sensors\n",
    "- Network problems in data transfer\n",
    "\n",
    "Data can be missing in three different ways (called different **missingness regimes**):\n",
    "1. MCAR - missing completely at random, a truly stochastic process (e.g., your thermomenter randomly fails to record one out of every hundred temperature readings)\n",
    "1. MAR - missing at random, a deterministic but noisy process that removes data based on other data (this is the single most common) (e.g., your thermometer fails when windspeeds are above 15 mph)\n",
    "1. MNAR - Missing not at random - a deterministic but noisy process that removes data based on the data itself (like your thermometer fails at temps above 75 degrees)\n",
    " \n",
    "What you *can't* do:\n",
    "1. Listwise deletion (aka `df.dropna()` in pandas). You can only do this if the data is missing completely at random, and this is impossible to prove theoretically and very unlikely in practice.\n",
    "1. Gather more data. You won’t be saved by big data. As a dataset with missing values gets larger and larger we get closer and closer to the wrong value for our coefficient estimates and this leads to false confidence.\n",
    "1. Just impute the mean. It doesnt matter how the data is missing, it will lead to larger bias in parameter estimates and larger model error.\n",
    " \n",
    "What you *can* do: \n",
    "1. Establish your missingness regime\n",
    "1. Use a modern multiple imputation technique like MICE (multiple imputation by chained equations)\n",
    "1. Use auxillary features… ie features you're not expecting to use in predition model, but you will use in imputation, because they are known to be strongly correlated with the missing data\n",
    "1. Anticipate extra compute time\n",
    "1. Report all the things as a good scientist and good citizen:\n",
    "  - How many rows are missing one or more values\n",
    "  - the fraction of missing values\n",
    "  - the pattern of missing values\n",
    "  - your imputation strategy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is EDA? or What do you do when you first get a dataset?\t....or Walk me through your EDA process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA stands for Exploratory Data Analysis  \n",
    "Fundamental categories of things I look at:\n",
    "- Basics\n",
    "- missingness\n",
    "- dispersion\n",
    "- outliers/anomalies - detection\n",
    "- correlation of features amonng each other\n",
    "- correlation of features with the target.\n",
    "\n",
    "My approach involves the following steps:\n",
    "1. Examine head and tail of dataframe, datatypes of columns, numbers of unique values per column\n",
    "1. Missingness:\n",
    " 1. Counts of missing (null/NaN) values\n",
    " 1. Create a datafarme of Boolean Missingness Flags\n",
    " 1. Examine pairwise correlations of missingness flag for all columns\n",
    "1. Split dataframe into continuous and categorical features\n",
    "  1. Possibly also deal with datetime and free-text, which are topics unto themselves.\n",
    "1. Continuous features:\n",
    " 1. Simple descriptive statistics (mean, median, min, max, quartiles, skewness) for each column\n",
    " 1. Correlation matrix (optionally visualized as a heatmap, ordered according to a hierarchical clustering algorithm.)\n",
    " 1. Outlier detection, using robust methods:\n",
    "    1. MAD - median absolute deviation\n",
    "    2. Mahalanobis distance\n",
    "  1. Visualize continuous distributions, using univariate and covariate plots:\n",
    "   1. Scattermatrix of subsets of continuous features (look at it before and after dropping outliers)\n",
    "   1. Strip plots and/or boxplots of single features of interest\n",
    "   1. Scatterplots of each continuous feature with the target\n",
    "1. Categorical features:\n",
    "  1. Bar charts representing value-counts of categorical features.\n",
    "  1. Look at frequency counts - consider ways of rolling up\n",
    "  1. Look at correlation w\n",
    "1. Maps, line plots with time, if you have a spatial or temporal problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, a data pipeline is an object that takes data as input and produces data as output and with a series of specific operations in between. There are *data processing* pipelines that perform Extract, Transform and Load (ETL) operations, and there are also *machine learning* pipelines that take raw features as input and produce predictions as as output. A machine learning pipeline may also include steps such as preprocessing, feature extraction, dimensionality reduction, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Informally, an index refers to the order in which data are organized for easy reference or access.\n",
    "\n",
    "In SQL-speak:\n",
    "Indexing means creating an index--a pointer to data in a table, a special lookup table that the database search engine can use to speed up data retrieval.\n",
    "\n",
    "In pandas-speak: \n",
    "Indexing, or reindexing a series (and by extention a dataframe), would be to assign alter the index, or axis labels, which are technically immutable n-dimensional arrays implementing an ordered, sliceable set. Rows and columns can be thought of as hash-like structures, where the keys are the index and the values are arrays. So indexing is specifiying the values to use as the keys, which we interact with as row labels and column labels. Indexes can be hierarchically structured using multi-indexes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
