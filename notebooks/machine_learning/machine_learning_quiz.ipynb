{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the top machine learning frameworks and libraries in python?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could look at Google Trends or Stack Overflow Tags, but here is my general sense:\n",
    "In python:\n",
    "- tensorflow\n",
    "- scikitlearn\n",
    "- sparkml\n",
    "- nltk\n",
    "- keras\n",
    "- pytorch\n",
    "\n",
    "There are other libraries are often *necessary* for machine learning projects, although I'm not sure if I would call them \"machine learning libraries\":\n",
    "- numpy\n",
    "- pandas\n",
    "- matplotlib/seaborn\n",
    "- scipy\n",
    "- statsmodels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised vs. Unsupervised Learning problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification vs. Regression Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curse of Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias/Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### entropy and information gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List at least 5 feature engineering and feature selection techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection Techniques:\n",
    "##### Filter Methods:\n",
    "- **Univariate** (SelectKbest): consider each feature individually and using test statistic (such as chi-squared or pearsons r, if the target is continuous), pick those features most closely associated with the target.\n",
    "- **Variance thresholding**: eliminate features with low levels of variance, because these cannot provide much signal to a model.\n",
    "- **Correlated Feature Elimination**: begin with the pairwise correlation coefficients of all features, sorted by the absolute value of their correlation coefficients. For the most correlated pair of features, remove the one that is less correlated on average with all other features. Recalculate average correlations and continue until no two features are correalted above some threshold (e.g., no two features have a correlation coefficient with an absolute value above .9).\n",
    "\n",
    "##### Wrapper Methods:\n",
    "- **Forward Selection**: Start with an empty set of features, add features one-by-one to reach an optimal model. **(flesh this out more)**\n",
    "- **Backward Elimination**: Start with the full set of features, remove features one-by-one to reach an optimal model. **(flesh this out more)**\n",
    "- **Recursive Feature Elimination**: Perform a greedy search to find the best performing feature subset. In the worst case, if a dataset contains N number of features RFE will do a greedy search for 2^N combinations of features.\n",
    "***\n",
    "#### Feature Engineering Techniques:\n",
    "- Create bins of continuous features\n",
    "- Perform group-by-aggregate operations\n",
    "- Create arithmetic feature interactions (e.g., new_feature = feature_A * feature_B)\n",
    "- Use a clustering algorithm to assign clusters of observations (being sure to exclude the target from this step)\n",
    "- For datetime, extract date parts (e.g., year, month, and day-of-week), or calculate ranges between dates.\n",
    "- If data are spatial, you can do a lot with basic geometry, such as using the physical distance to a nearest-neighbor\n",
    "- (There are endless possibilities, and domain knowledge should definitely play a role in suggesting new possible features.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a decision tree? How does a decision tree decide to make a split?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How would you go about building a bayesian model?\n",
    "### How does a random forest work?\n",
    "### How does gradient boosting work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Combining the results of multiple models to create more stable and accurate predictions.\n",
    "- Two examples of decision tree ensembles are random forests and gradient-boosted trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging and boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Both Bagging and Boosting are *ensemble techniques*.\n",
    "- Bagging is *parallel*, boosting is *sequential*.\n",
    "\n",
    "#### Bagging\n",
    "- Short for \"bootstrap aggregation\"\n",
    "- Divide the training set into multiple partitions, train models separately on each partition, and then use the average predictions from all models to make a final prediciton.\n",
    "\n",
    "#### Boosting\n",
    "- A model is first trained on all of the training data and predictions are made on the same data. \n",
    "- Predictions that the model got (the most) wrong are then \"up-weighted\", and then the model is trained again so it \"focuses\" on trying to improve the predicitons it got wrong in the previous iteration. \n",
    "- The process is repeated until some stopping condition is reached. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Plain English*: \n",
    "- Principal component analysis (PCA) is a method, taken from linear algebra, that provides a lower-dimensional representation of a dataset.\n",
    "\n",
    "*More technical*: \n",
    "- PCA takes a dataset (matrix)\n",
    "- performs an *orthogonal linear transformation that projects the matrix onto a new coordinate system such that the greatest variance is captured by the first coordinate* (the first principal component), the second-greatest variance is captured by the second coordinate, etc.\n",
    "- You can retain as many principal components as you want to reduce the dimensions arbitrarily.\n",
    "\n",
    "Uses: \n",
    "- *Reveal the latent structure* of a dataset in a way that best explains the variance within it. \n",
    "- *Decorrelate* the features within a dataset to mitigate problem of multicollinearity (while sacrificing some interpretability of the final model built from such a transformed dataset)\n",
    "- *Visualize* high-dimensional datasets, because the human eye can only see in three dimensions (and really only two--think of how difficult it is to read a 3D scatterplot, even if you've done it many times and can mainuplate it in space).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A technique for estimating the generalizability of a predictive model and detecting the problem of overfitting. \n",
    "- The classic approach is *k-folds cross validation*: \n",
    "  - Rather than randomly partition a dataset into a TRAINING set and a TEST set, we do this k times. \n",
    "  - In practice, k is usually set to 5 or 10, but this can vary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A Confusion Matrix is a specific type of *contingency table for evaluating the performance of a supervised classification algorithm*, so named because it shows if a model is confusing two classes (i.e., commonly mislabeling one as another). \n",
    "- The rows of the table show counts of the predicted class while the columns represent the instances of the actual class (or vice versa).\n",
    "- It gives a more fine-grained view than simple accuracy (the proportion of \"right answers\" out of all guesses) by showing *how* a model failed. \n",
    "- In the simple case with only two classes, positive and negative, the confusion matrix contains four quadrants, which represent counts of:\n",
    "  - True Positives\n",
    "  - False Positives\n",
    "  - True Negatives\n",
    "  - False Negatives\n",
    "\n",
    "#### Precision\n",
    "- TP / (TP + FP)\n",
    "- The fraction of records that were positive from the group that the classifier predicted to be positive\n",
    "- Also called positive predicted value\n",
    "\n",
    "#### Recall\n",
    "- TP / (TP + FN)\n",
    "- The fraction of positive examples the classifier got right\n",
    "- Also called sensitivity, hit rate, or true positive rate (used in ROC curve, see below)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC-AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Receiver Operator Characteristic - Area Under Curve** \n",
    "- Metric used in machine learning for evaluating and comparing supervised classification algorithms.  \n",
    "- A receiver operator characteristic curve is a graphical plot that shows the performance of a binary classifier as its discrimination threshold is varied. \n",
    "- The ROC curve is the **sensitivity** (True Positive Rate, or *recall*-- see above) as a function of **fall-out** (False Positive Rate, or 1 - specificity)\n",
    "- In theory, the area under the ROC curve varies between 0 and 1, with an uninformative classifier yielding 0.5, no better than random guessing. \n",
    "- A measurement of zero would represent the \"perverse\" case of the model always giving the wrong response, which would be a kind of 'inversely accurate prediction'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unbalanced dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Classification datasets can be “unbalanced\" when there are many observations of one class and few of another. \n",
    "- Accuracy-driven models will over-predict the majority class.\n",
    "  - If a dataset of credit card transactions is 99.9% NOT FRAUD and 0.1% FRAUD, then a model that makes a prediction of NOT FRAUD *every time* will be 99.9% accurate while missing every single case of fraud.\n",
    "\n",
    "#### Possible solutions:\n",
    "- **Downsample/Upsample**\n",
    "- **Cost-sensitive learning**: \n",
    "  - Incorporate the monetary costs of a \"miss\" into the objective function. \n",
    "  - For instance, in the context of fraud detection, there are relatively few fraud events, making an unbalanced dataset. \n",
    "  - We know however that the costs of a false negative are often much higher than the costs of a false positive\n",
    "    - i.e., failing to detect an instance of fraud can be more costly than incorrectly flagging a legitimate transaction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression\n",
    "- Take our features and multiply each by a weight and then sum them up\n",
    "- Feed this weighted sum into a sigmoid function, which will return a number between zero and one\n",
    "  - We can think of this as a probabilty estimate... the probability that the observation belongs to one of the classes\n",
    "- Anything above 0.5 we'll classify as 1 and anything below 0.5, we'll classify as 0.\n",
    "- Now we need to know what are the best weights or regression coefficients to use, and how to find them.\n",
    "  - This is an optimization problem which can be solved in many ways, including by stochastic gradient descent. (see below)\n",
    "  \n",
    "##### Stochastic gradient descent\n",
    "- Set all weights equal to 1\n",
    "- For each piece of data in the dataset:\n",
    " - Calculate the gradient of one piece of data\n",
    " - Update the weights vector by alpha * the gradient\n",
    " - return the weights vector\n",
    "\n",
    "**Pros**:\n",
    "- Computationally inexpensive\n",
    "- Easy to implement\n",
    "- Easy to interpret (we have coefficients, i.e., the weights)\n",
    "\n",
    "**Cons**\n",
    " - Prone to underfitting. May have low accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The support vector machine is a maximal margin classifier that seeks to construct a hyperplane that linearly separates training observations of one class from the other class(es).\n",
    " \n",
    "Preprocessing data for SVM\n",
    "Intuitively, it makes sense that SVMs might need scaling. Since SVMs are sensitive to the distance of points relative to a hyperplane, if one dimension had units in the thousands, the distance along that dimension would overwhelm another dimension with values in [0,1]. And the model would focus disproportionately on this larger dimension. Scaling overcomes this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN (K-nearest neighbors)\n",
    "The algorithm can be summarized as:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Receiver Operator Characteristic - Area Under Curve**. Metric used in machine learning for evaluating and comparing supervised classification algorithms.  A receiver operator characteristic curve is a graphical plot that shows the performance of a binary classifier as its discrimination threshold is varied. The ROC curve is the sensitivity (True Positive Rate) as a function of fall-out (False Positive Rate, 1 - specificity)\n",
    "\n",
    "In theory, the area under the ROC curve varies between 0 and 1, with an uninformative classifier yielding 0.5, no better than random guessing. A measurement of zero would represent the \"perverse\" case of the model always giving the wrong response, which would be a kind of 'inversely accurate prediction'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Hyperparameter optimization*: choosing a set of optimal hyperparameters for a machine learning algorithm. \n",
    "- A hyperparameter is a variable which controls the learning process (e.g., neuron weights or learning rates) rather than the patterns actually learned (i.e., parameters). \n",
    "- HPO is guided by some performance metric, typically measured by cross-validation on the training set. \n",
    "- One approach to HPO is *grid search*, which is an exhaustive search through all possible combinations of manually specified values for different hyperparameters.  \n",
    "  - It can result in combinatorial explosion, because it trains and evaluates a model for every combination of suggested values of the hyperparameters. \n",
    "  - However, it can usually be easily parallelized, because each model using a specific combination of hyperparameter values is typically independent of all other models built using different combinations.\n",
    "- An alternative approach is Bayesian Optimization, which operates sequentially, meaning that the hyperparameters chosen at the next step will be influenced by the performance of all the previous steps. Bayesian Optimization makes principled decisions about how to balance exploring new regions of the parameter space vs exploiting regions that are known to perform well. It’s generally much more efficient to use Bayesian Optimization than alternatives like Grid Search and Random Search.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, a data pipeline is an object that takes data as input and produces data as output and with a series of specific operations in between. There are *data processing* pipelines that perform Extract, Transform and Load (ETL) operations, and there are also *machine learning* pipelines that take raw features as input and produce predictions as as output. A machine learning pipeline may also include steps such as preprocessing, feature extraction, dimensionality reduction, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Informally, an index refers to the order in which data are organized for easy reference or access.\n",
    "\n",
    "In SQL-speak:\n",
    "Indexing means creating an index--a pointer to data in a table, a special lookup table that the database search engine can use to speed up data retrieval.\n",
    "\n",
    "In pandas-speak: \n",
    "Indexing, or reindexing a series (and by extention a dataframe), would be to assign alter the index, or axis labels, which are technically immutable n-dimensional arrays implementing an ordered, sliceable set. Rows and columns can be thought of as hash-like structures, where the keys are the index and the values are arrays. So indexing is specifiying the values to use as the keys, which we interact with as row labels and column labels. Indexes can be hierarchically structured using multi-indexes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is EDA? or What do you do when you first get a dataset?\t....or Walk me through your EDA process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA stands for Exploratory Data Analysis\n",
    "In broad terms: i think the fundamental things can be boiled down to:\n",
    "- missingness\n",
    "- dispersion\n",
    "- outliers/anomalies - detection\n",
    "- correlation of features amonng each other\n",
    "- correlation of features with the target.\n",
    "\n",
    "My approach involves the following steps:\n",
    "1. Examine head and tail of dataframe, datatypes of columns, numbers of unique values per column\n",
    "1. Missingness:\n",
    " 1. Counts of missing (null/NaN) values\n",
    " 1. Create a datafarme of Boolean Missingness Flags\n",
    " 1. Examine pairwise correlations of missingness flag for all columns\n",
    "1. Split dataframe into continuous and categorical features\n",
    "  1. Possibly also deal with datetime and free-text, which are topics unto themselves.\n",
    "1. Continuous features:\n",
    " 1. Simple descriptive statistics (mean, median, min, max, quartiles, skewness) for each column\n",
    " 1. Correlation matrix (optionally visualized as a heatmap, ordered according to a hierarchical clustering algorithm.)\n",
    " 1. Outlier detection, using robust methods:\n",
    "    1. MAD - median absolute deviation\n",
    "    2. Mahalanobis distance\n",
    "  1. Visualize continuous distributions, using univariate and covariate plots:\n",
    "   1. Scattermatrix of subsets of continuous features (look at it before and after dropping outliers)\n",
    "   1. Strip plots and/or boxplots of single features of interest\n",
    "   1. Scatterplots of each continuous feature with the target\n",
    "1. Categorical features:\n",
    "  1. Bar charts representing value-counts of categorical features.\n",
    "  1. Look at frequency counts - consider ways of rolling up\n",
    "  1. Look at correlation w\n",
    "1. Maps, line plots with time, if you have a spatial or temporal problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding means creating an abstract (mathematical) representation of some entity or phenomenon.\n",
    "\n",
    "For example:\n",
    "- You could encode a digital image as a matrix of RGB values for each pixel\n",
    "- You could encode categorical data using the one-hot encoding method, in which a single column with a finite number of categories (string values) is translated into several columns, one representing each possible category, with binary Boolean values (0 for False or 1 for True) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
