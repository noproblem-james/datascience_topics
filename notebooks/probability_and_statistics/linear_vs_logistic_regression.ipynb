{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized Linear Models Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's the difference between (OLS) Linear Regression and (Binomial) Logistic Regression?\n",
    "\n",
    "* In the linear regression model the dependent variable y is considered continuous, whereas in logistic regression it is categorical, i.e., discrete. \n",
    "\n",
    "\n",
    "* In application, the former is used in regression settings while the latter is used for binary classification or multi-class classification (where it is called multinomial logistic regression). \n",
    "\n",
    "#### Example:\n",
    "\n",
    "* Regression Problem: Predict sale price of houses based on size, where X is the area in square feet of houses, and Y is the corresponding price.\n",
    "\n",
    "\n",
    "* Classification Problem: Predict, based on size, whether a house would sell for more than 200K, where X is the area in square feet of houses and Y can take one of two possible values-- 1 for Yes, the house will sell for more than 200K, or 0 for No, the house will not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other of Differences\n",
    "\n",
    "\n",
    "#### Equation\n",
    "\n",
    "Linear Regression gives an equation which is of the form Y = mX + C, means equation with degree 1.However, Logistic Regression gives an equation which is of the form Y = e^X/1 + e^-X\n",
    "\n",
    "\n",
    "#### Coefficient interpretation\n",
    "\n",
    "In linear regression, the coefficient interpretation of independent variables are quite straight forward (i.e. holding all other variables constant, with an unit increase in this variable, the dependent variable is expected to increase/decrease by xxx). However in logistic regression, depends on the family (binomial, poisson, etc.) and link (log, logit, inverse-log, etc.) you use, the interpretation is different.\n",
    "\n",
    "#### Error Minimization Technique\n",
    "\n",
    "Linear Regression uses Ordinary Least Squares method to minimise the errors and arrive at a best possible fit while Logistic regression uses maximum likelihood method to arrive at the solution.\n",
    "\n",
    "Linear regression is usually solved by minimizing the least squares error of the model to the data, therefore large errors are penalized quadratically. Logistic regression is just the opposite. Using the logistic loss function causes large errors to be penalized to an asymptotic constant.\n",
    "\n",
    "Consider linear regression on a categorical {0,1} outcomes to see why this is a problem. If your model predicts the outcome is 38 when truth is 1, you've lost nothing. Linear regression would try to reduce that 38, logistic wouldn't (as much) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is Logistic Regression a type of Linear Regression?\n",
    "\n",
    "Logistic regression called a generalized linear model not because the estimated probability of the response event is linear, but because the logit of the estimated probability response is a linear function of the parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare and Contrast: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  | Linear Regression | Logistic Regression |\n",
    "|  | Supervised <br> Parametric <br> **Regression** | Supervised <br> Parametric <br> **Classification** |\n",
    "| Loss Function (How wrong is model, pt-by-pt?) | Mean squared error <br> Also absolute loss (not often used)<br> Regularization (AKA shrinkage):Ridge Regression (closed-form) LASSO (no closed-form) | Log loss |\n",
    "| Parameters | **Parameters**: Coefficients (i) <br> (change in response for every one-unit change in predictor) <br> ** Hyperparamters** : Penalty factor Œª(if using regularization) | **Parameters**: Coefficients (ùõΩi) <br> Represent log odds (log of the odds ratio) |\n",
    "| Solver/Opt strats | Closed form... Or: Gradient descent | Gradient descent |\n",
    "| Pros | Computationally inexpensive  <br> Easy to implement <br> Easy to interpret results | Computationally inexpensive <br> Easy to implement <br> Easy to interpret (if you understand log loss) |\n",
    "| Cons | Poorly models nonlinear data If doing inference, need to check violation of **assumptions**: <br> Normal distribution of residuals <br> constant variance of error terms <br> ind of error terms from predictors <br> I.I.D. (random sample), <br> endogenous predictors | Prone to underfitting <br>May have low accuracy (?) <br> If doing inference, need to check violation of **assumptions** (not exactly the same as linear models) <br>  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions that do not apply to logistic regresssion:\n",
    "* linear relationship between the dependent and independent variables.\n",
    "* independent variables do not need to be multivariate normal ‚Äì although multivariate normality yields a more stable solution. \n",
    "* homoscedasticity is not needed.\n",
    "* independent variables do not need to be metric (interval or ratio scaled)\n",
    "\n",
    "### Assumptions that do apply to logistic regression:\n",
    "* dependent variable must be binary (or ordinal)\n",
    "* error terms need to be independent\n",
    "* little or no multicollinearity\n",
    "* independent variables are linearly related to the log odds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logit(pi)=ln(pi1‚àípi)=Œ≤0+Œ≤1x1,i+Œ≤2x2,i+‚ãØ+Œ≤pxp,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "Can think of regularization as adding (or increasing the) bias if our model suffers from (high) variance (i.e., it overfits the training data). On the other hand, too much bias will result in underfitting (a characteristic indicator of high bias is that the model shows a \"bad\" performance for both the training and test dataset). We know that our goal in an unregularized model is to minimize the cost function, i.e., we want to find the feature weights that correspond to the global cost minimum (remember that the logistic cost function is convex).  \n",
    "  \n",
    "  Now, if we regularize the cost function (e.g., via L2 regularization), we add an additional to our cost function (J) that increases as the value of your parameter weights (w) increase; keep in mind that the regularization we add a new hyperparameter, lambda, to control the regularization strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.plotly\n",
    "import plotly.graph_objs as go\n",
    "import scipy.stats as stats\n",
    "from sklearn import linear_model\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jameshelfrich/miniconda3/envs/my_env/lib/python3.7/site-packages/plotly/graph_objs/_deprecations.py:426: DeprecationWarning:\n",
      "\n",
      "plotly.graph_objs.Marker is deprecated.\n",
      "Please replace it with one of the following more specific types\n",
      "  - plotly.graph_objs.scatter.Marker\n",
      "  - plotly.graph_objs.histogram.selected.Marker\n",
      "  - etc.\n",
      "\n",
      "\n",
      "/Users/jameshelfrich/miniconda3/envs/my_env/lib/python3.7/site-packages/plotly/graph_objs/_deprecations.py:318: DeprecationWarning:\n",
      "\n",
      "plotly.graph_objs.Font is deprecated.\n",
      "Please replace it with one of the following more specific types\n",
      "  - plotly.graph_objs.layout.Font\n",
      "  - plotly.graph_objs.layout.hoverlabel.Font\n",
      "  - etc.\n",
      "\n",
      "\n",
      "/Users/jameshelfrich/miniconda3/envs/my_env/lib/python3.7/site-packages/plotly/graph_objs/_deprecations.py:144: DeprecationWarning:\n",
      "\n",
      "plotly.graph_objs.Annotation is deprecated.\n",
      "Please replace it with one of the following more specific types\n",
      "  - plotly.graph_objs.layout.Annotation\n",
      "  - plotly.graph_objs.layout.scene.Annotation\n",
      "\n",
      "\n",
      "/Users/jameshelfrich/miniconda3/envs/my_env/lib/python3.7/site-packages/plotly/graph_objs/_deprecations.py:531: DeprecationWarning:\n",
      "\n",
      "plotly.graph_objs.XAxis is deprecated.\n",
      "Please replace it with one of the following more specific types\n",
      "  - plotly.graph_objs.layout.XAxis\n",
      "  - plotly.graph_objs.layout.scene.XAxis\n",
      "\n",
      "\n",
      "/Users/jameshelfrich/miniconda3/envs/my_env/lib/python3.7/site-packages/plotly/graph_objs/_deprecations.py:558: DeprecationWarning:\n",
      "\n",
      "plotly.graph_objs.YAxis is deprecated.\n",
      "Please replace it with one of the following more specific types\n",
      "  - plotly.graph_objs.layout.YAxis\n",
      "  - plotly.graph_objs.layout.scene.YAxis\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~noproblem-james/13.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<chart_studio.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Learn about API authentication here: https://plot.ly/python/getting-started\n",
    "# Find your api_key here: https://plot.ly/settings/api\n",
    "xi = np.arange(0,9)\n",
    "A = np.array([ xi, np.ones(9)])\n",
    "\n",
    "# (Almost) linear sequence\n",
    "y = [19, 20, 20.5, 21.5, 22, 23, 23, 25.5, 24]\n",
    "\n",
    "# Generated linear fit\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(xi,y)\n",
    "line = slope*xi+intercept\n",
    "\n",
    "# Creating the dataset, and generating the plot\n",
    "trace1 = go.Scatter(\n",
    "                  x=xi,\n",
    "                  y=y,\n",
    "                  mode='markers',\n",
    "                  marker=go.Marker(color='rgb(255, 127, 14)'),\n",
    "                  name='Data'\n",
    "                  )\n",
    "\n",
    "trace2 = go.Scatter(\n",
    "                  x=xi,\n",
    "                  y=line,\n",
    "                  mode='lines',\n",
    "                  marker=go.Marker(color='rgb(31, 119, 180)'),\n",
    "                  name='Fit'\n",
    "                  )\n",
    "\n",
    "annotation = go.Annotation(\n",
    "                  x=3.5,\n",
    "                  y=24.5,\n",
    "                  text='$R^2 = 0.9551,\\\\Y = 0.716X + 19.18$',\n",
    "                  showarrow=False,\n",
    "                  font=go.Font(size=16)\n",
    "                  )\n",
    "layout = go.Layout(\n",
    "                title='Linear Fit in Python',\n",
    "                plot_bgcolor='rgb(229, 229, 229)',\n",
    "                  xaxis=go.XAxis(zerolinecolor='rgb(255,255,255)', gridcolor='rgb(255,255,255)'),\n",
    "                  yaxis=go.YAxis(zerolinecolor='rgb(255,255,255)', gridcolor='rgb(255,255,255)'),\n",
    "                  annotations=[annotation]\n",
    "                )\n",
    "\n",
    "data = [trace1, trace2]\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "plotly.plotly.iplot(fig, filename='Linear-Fit-in-python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jameshelfrich/miniconda3/envs/my_env/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning:\n",
      "\n",
      "Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is our test set, it's just a straight line with some\n",
    "# Gaussian noise\n",
    "xmin, xmax = -5, 5\n",
    "n_samples = 100\n",
    "np.random.seed(0)\n",
    "X = np.random.normal(size=n_samples)\n",
    "y = (X > 0).astype(np.float)\n",
    "X[X > 0] *= 4\n",
    "X += .3 * np.random.normal(size=n_samples)\n",
    "\n",
    "X = X[:, np.newaxis]\n",
    "# run the classifier\n",
    "clf = linear_model.LogisticRegression(C=1e5)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = go.Scatter(x=X.ravel(), y=y, \n",
    "                mode='markers',\n",
    "                marker=dict(color='black'),\n",
    "                showlegend=False\n",
    "               )\n",
    "X_test = np.linspace(-5, 10, 300)\n",
    "\n",
    "def model(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "loss = model(X_test * clf.coef_ + clf.intercept_).ravel()\n",
    "\n",
    "p2 = go.Scatter(x=X_test, y=loss, \n",
    "                mode='lines',\n",
    "                line=dict(color='red', width=3),\n",
    "                name='Logistic Regression Model')\n",
    "\n",
    "ols = linear_model.LinearRegression()\n",
    "ols.fit(X, y)\n",
    "\n",
    "p3 = go.Scatter(x=X_test, y=ols.coef_ * X_test + ols.intercept_, \n",
    "                mode='lines',\n",
    "                line=dict(color='blue', width=1),\n",
    "                name='Linear Regression Model'\n",
    "                )\n",
    "p4 = go.Scatter(x=[-4, 10], y=2*[.5],\n",
    "                mode='lines',\n",
    "                line=dict(color='gray', width=1),\n",
    "                showlegend=False\n",
    "               )\n",
    "\n",
    "layout = go.Layout(xaxis=dict(title='x', range=[-4, 10],\n",
    "                              zeroline=False),\n",
    "                   yaxis=dict(title='y', range=[-0.25, 1.25],\n",
    "                              zeroline=False))\n",
    "\n",
    "fig = go.Figure(data=[p1, p2, p3, p4], layout=layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jameshelfrich/miniconda3/envs/my_env/lib/python3.7/site-packages/IPython/core/display.py:689: UserWarning:\n",
      "\n",
      "Consider using IPython.display.IFrame instead\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~noproblem-james/24.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<chart_studio.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's tempting to use the linear regression output as probabilities but it's a mistake because the output can be negative, and greater than 1 whereas probability can not. Logistic regression exists because a standard linear regression could produce probabilities less than 0 or larger than 1. Also, linear regression is more sensitive to outliers than logistic regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
